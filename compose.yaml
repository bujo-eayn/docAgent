# compose.yaml

services:
  # Ollama service for running LLM models > Removed for now due to error on Windows
  # ollama:
  #   image: ollama/ollama:latest
  #   container_name: docAgent-ollama
  #   volumes:
  #     - ollama_data:/root/.ollama
  #   ports:
  #     - "11434:11434"
  #   networks:
  #     - docAgent-network
  #   healthcheck:
  #     test: ["CMD-SHELL", "curl -f http://localhost:11434/ || exit 1"]
  #     interval: 10s
  #     timeout: 5s
  #     retries: 10
  #     start_period: 30s
    # Comment out GPU support for Windows (unless you have NVIDIA GPU with proper drivers)
    # deploy:
    #   resources:
    #     reservations:
    #       devices:
    #         - driver: nvidia
    #           count: all
    #           capabilities: [gpu]

  # Backend FastAPI service
  backend:
    build:
      context: .
      dockerfile: Dockerfile
    container_name: docAgent-backend
    volumes:
      - ./data:/app/data
      - ./app:/app/app
      - ./config.py:/app/config.py
      - ./models.py:/app/models.py
    ports:
      - "8000:8000"
    environment:
      - OLLAMA_URL=http://host.docker.internal:11434
      - MODEL_NAME=gemma3
      - DATA_DIR=/app/data
    depends_on:
      ollama:
        condition: service_started
    networks:
      - docAgent-network
    restart: unless-stopped

  # Frontend Streamlit service
  frontend:
    build:
      context: .
      dockerfile: Dockerfile.frontend
    container_name: docAgent-frontend
    volumes:
      - ./home.py:/app/home.py
    ports:
      - "8501:8501"
    environment:
      - BACKEND_URL=http://backend:8000
    depends_on:
      - backend
    networks:
      - docAgent-network
    restart: unless-stopped

  # Model initialization service (runs once to download models) > Removed for now due to error on Windows
  # model-init:
  #   image: ollama/ollama:latest
  #   container_name: docAgent-model-init
  #   volumes:
  #     - ollama_data:/root/.ollama
  #   networks:
  #     - docAgent-network
  #   depends_on:
  #     ollama:
  #       condition: service_started
  #   entrypoint: ["/bin/sh", "-c"]
  #   command:
  #     - |
  #       echo "Waiting for Ollama to be ready..."
  #       sleep 10
  #       echo "Pulling gemma3 model..."
  #       ollama pull gemma3
  #       echo "Pulling mxbai-embed-large model..."
  #       ollama pull mxbai-embed-large
  #       echo "Models downloaded successfully!"
  #   restart: "no"

networks:
  docAgent-network:
    driver: bridge

volumes:
  ollama_data:
    driver: local
